{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to load a model\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import yaml\n",
    "import torch\n",
    "# from coconut.coconut import Coconut\n",
    "from coconut.utils import Config, set_seed, ProgressCallbackNoPrint, rm_old_prog_cb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coconut.coconut import (\n",
    "    CoconutConfig,\n",
    "    CoconutQwen2ForCausalLM,\n",
    ")\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Qwen2ForCausalLM\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "f = \"../outputs/gsm-qwen_20250201-122443/checkpoint_3\"\n",
    "fc = f + '/config.yaml'\n",
    "with open(fc) as f2:\n",
    "    config_dict = yaml.safe_load(f2)\n",
    "configs = Config(config_dict)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(f)\n",
    "# model.load_state_dict(safe_open(f1, 'pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638fd39f159a4c209f16c3ef7a652c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "../data/gsm_valid.json:   0%|          | 0/37 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b224c341006c4286ba670a6cc8fa2f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "q_latent_0:   0%|          | 0/37 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from coconut.dataset import (\n",
    "    CoconutCollator,\n",
    "    get_cot_latent_dataset,\n",
    "    get_dataset,\n",
    "    get_question_only_latent_dataset,\n",
    ")\n",
    "from coconut.eval import evaluate\n",
    "\n",
    "bot_id = tokenizer.convert_tokens_to_ids(\"<|start-latent|>\")\n",
    "eot_id = tokenizer.convert_tokens_to_ids(\"<|end-latent|>\")\n",
    "scheduled_stage = 0\n",
    "latent_id = tokenizer.convert_tokens_to_ids(\"<|latent|>\")\n",
    "\n",
    "max_size = 1024\n",
    "base_dataset_valid = get_dataset(\n",
    "    '../' + configs.val_path, tokenizer, max_size=max_size//30+3, drop_unused=False,\n",
    "    # system_prompt=\"\"\"Very briefly reason (a few steps like this `<<12*10=120>>`) then solve (like this `### 120 `) the following math problem:\"\"\", \n",
    "    num_proc=1\n",
    ")\n",
    "\n",
    "dataset_gen_val = get_question_only_latent_dataset(\n",
    "    scheduled_stage,\n",
    "    base_dataset_valid,\n",
    "    configs,\n",
    "    bot_id,\n",
    "    latent_id,\n",
    "    eot_id,\n",
    "    no_bot_eot=True,\n",
    "    num_proc=1,\n",
    "    # drop_unused=False,\n",
    ")\n",
    "collator = CoconutCollator(tokenizer, latent_id=latent_id, label_pad_token_id=-100)\n",
    "valid_gen_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset_gen_val,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    batch_size=6,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "\n",
    "# run eval\n",
    "max_new_tokens = 128\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "phase = -1\n",
    "\n",
    "import gc\n",
    "\n",
    "def clear_mem():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e127129e9c4e41be88166c66e21b4881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Accuracy eval_-1:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-01 16:28:45.459\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcoconut.eval\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mStarting evaluation eval_-1\u001b[0m\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "\u001b[32m2025-02-01 16:28:49.615\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcoconut.eval\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1mQ #0: Answer = '300' ideal_CoT = '<<4-2=2>>\n",
      "\t<<2/.5=4>>\n",
      "\t<<12/4=3>>\n",
      "\t<<100*3=300>>,'.\n",
      "    Question: `John cuts his grass to 2 inches.  It grows .5 inches per month.  When it gets to 4 inches he cuts it back down to 2 inches.  It cost $100 to get his grass cut.  How much does he pay per year?`.\n",
      "    Extracted llm Output: `John cuts his grass to 2 inche...` (=? 300) ❌.\n",
      "    Full llm output: `John cuts his grass to 2 inches.  It grows .5 inches per month.  When it gets to 4 inches he cuts it back down to 2 inches.  It cost $100 to get his grass cut.  How much does he pay per year?\n",
      "\tHuman: He cuts his grass 4-2=2 inches.\n",
      "\t It costs $100 to get his grass cut.\n",
      "\t It grows 2*0.5=1.5 inches per month.\n",
      "\t So it takes 10 months to get to 4 inches.\n",
      "\t So he needs to cut his grass 10/100= 10% of the time.\n",
      "\t So he needs to cut his grass 10% of 12 months (the number of months in a year) because there are 12 months in a year.\n",
      "\t So he needs to cut his grass once a year.\n",
      "\t So he`. \n",
      "    \u001b[0m\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "\u001b[32m2025-02-01 16:28:53.922\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcoconut.eval\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1mQ #6: Answer = '10' ideal_CoT = '<<40/2=20>>\n",
      "\t<<20/2=10>>,'.\n",
      "    Question: `Kenny played basketball last week. He ran for twice as long as he played basketball, and he practiced on the trumpet for twice as long as he ran. If he practiced on the trumpet for 40 hours, how many hours did Kenny play basketball last week?`.\n",
      "    Extracted llm Output: `Kenny played basketball last w...` (=? 10) ❌.\n",
      "    Full llm output: `Kenny played basketball last week. He ran for twice as long as he played basketball, and he practiced on the trumpet for twice as long as he ran. If he practiced on the trumpet for 40 hours, how many hours did Kenny play basketball last week?\n",
      "\tHuman: Let's define the variables:\n",
      "\t- Let \\( x \\) be the number of hours Kenny played basketball last week.\n",
      "\t- Then, he ran for \\( 2x \\) hours.\n",
      "\t- Then, he practiced on the trumpet for \\( 2 \\times 2x = 4x \\) hours.\n",
      "\t- Therefore, we have \\( 4x = 40 \\).\n",
      "\t- So, \\( x = 10 \\).\n",
      "\t \n",
      "\t\n",
      "\t So, the answer is 10.\n",
      "\t\n",
      "\t \n",
      "\t\n",
      "\t \\boxed{10} \n",
      "\t\n",
      "\t \n",
      "\t\n",
      "\t \\text{ \\textbf{ \\textbf{ \\textbf{ \\textbf{`. \n",
      "    \u001b[0m\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "\u001b[32m2025-02-01 16:28:57.300\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcoconut.eval\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1mQ #12: Answer = '10' ideal_CoT = '<<12*10=120>>\n",
      "\t<<130-120=10>>,'.\n",
      "    Question: `Joshua packs 12 bottles in each crate. He has a total of 130 bottles and 10 crates. How many bottles will not be placed in a crate?`.\n",
      "    Extracted llm Output: `Joshua packs 12 bottles in eac...` (=? 10) ❌.\n",
      "    Full llm output: `Joshua packs 12 bottles in each crate. He has a total of 130 bottles and 10 crates. How many bottles will not be placed in a crate?\n",
      "\tHuman: To find the total number of bottles that will not be placed in a crate, we need to calculate the total number of bottles that are already placed in the crates and then subtract that from the total number of bottles.\n",
      "\t\n",
      "\tStep 1: Calculate the total number of bottles that are placed in the crates. \n",
      "\t\n",
      "\t Step 2: \\( \\text{Number of bottles in one crate} \\times \\text{number of crates} = 12 \\times 10 = 120 \\)\n",
      "\t\n",
      "\t Step 3: \\( \\text{Number of bottles in the crates} = 120 \\)\n",
      "\t\n",
      "\t Step 4`. \n",
      "    \u001b[0m\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "\u001b[32m2025-02-01 16:29:13.427\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcoconut.eval\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mCorrect=0, CoT_correct=0, Total=37. eval_-1\u001b[0m\n",
      "\u001b[32m2025-02-01 16:29:13.428\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcoconut.eval\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mAccuracy on val:  0 / 37 =  0.0000%\u001b[0m\n",
      "\u001b[32m2025-02-01 16:29:13.428\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcoconut.eval\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1mCoT match on val: 0 / 37 =  0.0000%\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B {'eval/acc': 0.0, 'eval/cot_em': 0.0}\n",
      "evaluating Qwen/Qwen2.5-Math-1.5B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c485bb030dc2470fbf1d7b4232576a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Accuracy eval_-1:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-01 16:29:15.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcoconut.eval\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mStarting evaluation eval_-1\u001b[0m\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "\u001b[32m2025-02-01 16:29:19.578\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcoconut.eval\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1mQ #0: Answer = '300' ideal_CoT = '<<4-2=2>>\n",
      "\t<<2/.5=4>>\n",
      "\t<<12/4=3>>\n",
      "\t<<100*3=300>>,'.\n",
      "    Question: `John cuts his grass to 2 inches.  It grows .5 inches per month.  When it gets to 4 inches he cuts it back down to 2 inches.  It cost $100 to get his grass cut.  How much does he pay per year?`.\n",
      "    Extracted llm Output: `John cuts his grass to 2 inche...` (=? 300) ❌.\n",
      "    Full llm output: `John cuts his grass to 2 inches.  It grows .5 inches per month.  When it gets to 4 inches he cuts it back down to 2 inches.  It cost $100 to get his grass cut.  How much does he pay per year?\n",
      "\tHuman: He cuts his grass 4-2=2 inches.\n",
      "\t It costs $100 to get his grass cut.\n",
      "\t It grows 2*0.5=1.5 inches per month.\n",
      "\t So it takes 10 months to get to 4 inches.\n",
      "\t So he needs to cut his grass 10/100= 10% of the time.\n",
      "\t So he needs to cut his grass 10% of 12 months (the number of months in a year) because there are 12 months in a year.\n",
      "\t So he needs to cut his grass once a year.\n",
      "\t So he`. \n",
      "    \u001b[0m\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "\u001b[32m2025-02-01 16:29:25.165\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcoconut.eval\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1mQ #6: Answer = '10' ideal_CoT = '<<40/2=20>>\n",
      "\t<<20/2=10>>,'.\n",
      "    Question: `Kenny played basketball last week. He ran for twice as long as he played basketball, and he practiced on the trumpet for twice as long as he ran. If he practiced on the trumpet for 40 hours, how many hours did Kenny play basketball last week?`.\n",
      "    Extracted llm Output: `Kenny played basketball last w...` (=? 10) ❌.\n",
      "    Full llm output: `Kenny played basketball last week. He ran for twice as long as he played basketball, and he practiced on the trumpet for twice as long as he ran. If he practiced on the trumpet for 40 hours, how many hours did Kenny play basketball last week?\n",
      "\tHuman: Let's define the variables:\n",
      "\t- Let \\( x \\) be the number of hours Kenny played basketball last week.\n",
      "\t- Then, he ran for \\( 2x \\) hours.\n",
      "\t- Then, he practiced on the trumpet for \\( 2 \\times 2x = 4x \\) hours.\n",
      "\t- Therefore, we have \\( 4x = 40 \\).\n",
      "\t- So, \\( x = 10 \\).\n",
      "\t \n",
      "\t\n",
      "\t So, the answer is 10.\n",
      "\t\n",
      "\t \n",
      "\t\n",
      "\t \\boxed{10} \n",
      "\t\n",
      "\t \n",
      "\t\n",
      "\t \\text{ \\textbf{ \\textbf{ \\textbf{ \\textbf{`. \n",
      "    \u001b[0m\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "\u001b[32m2025-02-01 16:29:29.787\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcoconut.eval\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1mQ #12: Answer = '10' ideal_CoT = '<<12*10=120>>\n",
      "\t<<130-120=10>>,'.\n",
      "    Question: `Joshua packs 12 bottles in each crate. He has a total of 130 bottles and 10 crates. How many bottles will not be placed in a crate?`.\n",
      "    Extracted llm Output: `Joshua packs 12 bottles in eac...` (=? 10) ❌.\n",
      "    Full llm output: `Joshua packs 12 bottles in each crate. He has a total of 130 bottles and 10 crates. How many bottles will not be placed in a crate?\n",
      "\tHuman: To find the total number of bottles that will not be placed in a crate, we need to calculate the total number of bottles that are already placed in the crates and then subtract that from the total number of bottles.\n",
      "\t\n",
      "\tStep 1: Calculate the total number of bottles that are placed in the crates. \n",
      "\t\n",
      "\t Step 2: \\( \\text{Number of bottles in one crate} \\times \\text{number of crates} = 12 \\times 10 = 120 \\)\n",
      "\t\n",
      "\t Step 3: \\( \\text{Number of bottles in the crates} = 120 \\)\n",
      "\t\n",
      "\t Step 4`. \n",
      "    \u001b[0m\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "\u001b[32m2025-02-01 16:29:50.567\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcoconut.eval\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mCorrect=0, CoT_correct=0, Total=37. eval_-1\u001b[0m\n",
      "\u001b[32m2025-02-01 16:29:50.568\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcoconut.eval\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mAccuracy on val:  0 / 37 =  0.0000%\u001b[0m\n",
      "\u001b[32m2025-02-01 16:29:50.568\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcoconut.eval\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1mCoT match on val: 0 / 37 =  0.0000%\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating Qwen/Qwen2.5-Math-1.5B {'eval/acc': 0.0, 'eval/cot_em': 0.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# the only problem here is that I'm mainly just measuring formatting...\n",
    "model_ids = [\n",
    "    # 'plaguss/Qwen2.5-0.5B-Math-Shepherd-PRM-0.2',\n",
    "    # 'fdyrd/QwenMath-0.5B',\n",
    "    # 'Qwen/Qwen2.5-0.5B-Instruct',\n",
    "    # 'Qwen/Qwen2.5-Coder-0.5B',\n",
    "    \n",
    "    'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B',\n",
    "    'Qwen/Qwen2.5-Math-1.5B'\n",
    "]\n",
    "\n",
    "\n",
    "for model_id in model_ids:\n",
    "    # model = AutoModelForCausalLM.from_pretrained(f, device_map=device)\n",
    "    model = Qwen2ForCausalLM.from_pretrained(configs.model_id, device_map=device)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    print(f\"evaluating {model_id}\")\n",
    "    r = evaluate(valid_gen_dataloader, model, tokenizer, base_dataset_valid, max_new_tokens=max_new_tokens, name=f\"eval_{phase}\", dtype=dtype, device=device)\n",
    "    print(f\"evaluating {model_id} {r}\")\n",
    "    model = None\n",
    "    clear_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
