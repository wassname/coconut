{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to load a model\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import yaml\n",
    "import torch\n",
    "# from coconut.coconut import Coconut\n",
    "from coconut.utils import Config, set_seed, ProgressCallbackNoPrint, rm_old_prog_cb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coconut.coconut import (\n",
    "    CoconutConfig,\n",
    "    CoconutQwen2ForCausalLM,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "f = \"../outputs/gsm-qwen_20250201-122443/checkpoint_3\"\n",
    "fc = f + '/config.yaml'\n",
    "with open(fc) as f2:\n",
    "    config_dict = yaml.safe_load(f2)\n",
    "configs = Config(config_dict)\n",
    "\n",
    "model = CoconutQwen2ForCausalLM.from_pretrained(f, device_map=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(f)\n",
    "# model.load_state_dict(safe_open(f1, 'pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10fe3d5c9af446b0a140c165a417bda5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "../data/gsm_valid.json (num_proc=32):   0%|          | 0/37 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e988ffd275745099a9fd33e252825c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "q_latent_3 (num_proc=32):   0%|          | 0/37 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "587c87fcd81c42e798ef3d6a864c1ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Accuracy eval_-1:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-01 14:21:41.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcoconut.eval\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mStarting evaluation eval_-1\u001b[0m\n",
      "\u001b[32m2025-02-01 14:21:43.164\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcoconut.eval\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mQ #0: Answer = '300' ideal_CoT = '<<4-2=2>>\n",
      "\t<<2/.5=4>>\n",
      "\t<<12/4=3>>\n",
      "\t<<100*3=300>>,'.\n",
      "Question: `John cuts his grass to 2 inches.  It grows .5 inches per month.  When it gets to 4 inches he cuts it back down to 2 inches.  It cost $100 to get his grass cut.  How much does he pay per year?`.\n",
      "Extracted llm Output: `1600` (=? 300) ❌.\n",
      "Full llm output: `John cuts his grass to 2 inches.  It grows .5 inches per month.  When it gets to 4 inches he cuts it back down to 2 inches.  It cost $100 to get his grass cut.  How much does he pay per year?\n",
      "\t<|start-latent|><|latent|><|latent|><|latent|><|latent|><|latent|><|latent|><|end-latent|><<16*100=1600>>\n",
      "\t### 1600`. \n",
      "\u001b[0m\n",
      "\u001b[32m2025-02-01 14:21:43.402\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcoconut.eval\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mQ #1: Answer = '10' ideal_CoT = '<<1.5*2=3>>\n",
      "\t<<3+2.5=5.5>>\n",
      "\t<<1.5+3+5.5=10>>,'.\n",
      "Question: `Hannah has three dogs. The first dog eats 1.5 cups of dog food a day. The second dog eats twice as much while the third dog eats 2.5 cups more than the second dog. How many cups of dog food should Hannah prepare in a day for her three dogs?`.\n",
      "Extracted llm Output: `10.5` (=? 10) ❌.\n",
      "Full llm output: `Hannah has three dogs. The first dog eats 1.5 cups of dog food a day. The second dog eats twice as much while the third dog eats 2.5 cups more than the second dog. How many cups of dog food should Hannah prepare in a day for her three dogs?\n",
      "\t<|start-latent|><|latent|><|latent|><|latent|><|latent|><|latent|><|latent|><|end-latent|>### 10.5`. \n",
      "\u001b[0m\n",
      "\u001b[32m2025-02-01 14:21:43.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcoconut.eval\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mQ #2: Answer = '1400' ideal_CoT = '<<30/100*2000=600>>\n",
      "\t<<2000-600=1400>>,'.\n",
      "Question: `Travis wants to fly to Australia. The regular tickets cost about $2000. As Travis is a student, he will get a 30% discount on this price. How much does he need to pay for his ticket?`.\n",
      "Extracted llm Output: `1400` (=? 1400) ✅.\n",
      "Full llm output: `Travis wants to fly to Australia. The regular tickets cost about $2000. As Travis is a student, he will get a 30% discount on this price. How much does he need to pay for his ticket?\n",
      "\t<|start-latent|><|latent|><|latent|><|latent|><|latent|><|latent|><|latent|><|end-latent|>### 1400`. \n",
      "\u001b[0m\n",
      "\u001b[32m2025-02-01 14:21:54.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcoconut.eval\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mCorrect=2, CoT_correct=0, Total=37. eval_-1\u001b[0m\n",
      "\u001b[32m2025-02-01 14:21:54.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcoconut.eval\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mAccuracy on validation set:  2 / 37 = 0.05405405405405406\u001b[0m\n",
      "\u001b[32m2025-02-01 14:21:54.121\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcoconut.eval\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mCoT match on validation set: 0 / 37 = 0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from coconut.dataset import (\n",
    "    CoconutCollator,\n",
    "    get_cot_latent_dataset,\n",
    "    get_dataset,\n",
    "    get_question_only_latent_dataset,\n",
    ")\n",
    "from coconut.eval import evaluate\n",
    "\n",
    "bot_id = tokenizer.convert_tokens_to_ids(\"<|start-latent|>\")\n",
    "eot_id = tokenizer.convert_tokens_to_ids(\"<|end-latent|>\")\n",
    "scheduled_stage = 3\n",
    "latent_id = tokenizer.convert_tokens_to_ids(\"<|latent|>\")\n",
    "\n",
    "max_size = 1024\n",
    "base_dataset_valid = get_dataset(\n",
    "    '../' + configs.val_path, tokenizer, max_size=max_size//30+3, drop_unused=False\n",
    ")\n",
    "\n",
    "dataset_gen_val = get_question_only_latent_dataset(\n",
    "    scheduled_stage,\n",
    "    base_dataset_valid,\n",
    "    configs,\n",
    "    bot_id,\n",
    "    latent_id,\n",
    "    eot_id,\n",
    "    no_bot_eot=False,\n",
    "    # drop_unused=False,\n",
    ")\n",
    "collator = CoconutCollator(tokenizer, latent_id=latent_id, label_pad_token_id=-100)\n",
    "valid_gen_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset_gen_val,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    "    batch_size=1,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "\n",
    "# run eval\n",
    "max_new_tokens = 64\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "phase = -1\n",
    "\n",
    "r = evaluate(valid_gen_dataloader, model, tokenizer, base_dataset_valid, max_new_tokens=max_new_tokens, name=f\"eval_{phase}\", dtype=dtype, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
